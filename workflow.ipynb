{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a700bc78-c29a-4b8e-8f80-d98de97c1cb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed647ca2-a3c2-4280-9f59-9666800ea72f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import glob\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a553b00-18cd-4788-9b28-ae21aa3b4e4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_CACHE_DIR = \"data\"\n",
    "data_dir = os.path.join(DATA_CACHE_DIR, 'prachathai67k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d29b753-3f27-4c3e-b2eb-0e936a7584ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
    "# os.makedirs(data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01704a85-cbe9-40da-9598-0219c6a9b48b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_file(url: str, fname: str, chunk_size=1024):\n",
    "    \n",
    "    resp = requests.get(url, stream=True)\n",
    "    total = int(resp.headers.get(\"content-length\", 0))\n",
    "    with open(fname, \"wb\") as file, tqdm(\n",
    "            desc=fname,\n",
    "            total=total,\n",
    "            unit=\"iB\",\n",
    "            unit_scale=True,\n",
    "            unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        for data in resp.iter_content(chunk_size=chunk_size):\n",
    "            size = file.write(data)\n",
    "            bar.update(size)\n",
    "            \n",
    "def download():\n",
    "    os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
    "    \n",
    "    data_url = \"https://archive.org/download/prachathai67k/data.zip\"\n",
    "    data_filename = os.path.join(DATA_CACHE_DIR, \"data.zip\")\n",
    "    if not os.path.exists(data_filename):\n",
    "        print(f\"Downloading {data_url} to {data_filename}...\")\n",
    "        download_file(data_url, data_filename)\n",
    "    else:\n",
    "        print(f\"{data_filename} already exists, skipping download...\")\n",
    "    \n",
    "    data_dir = os.path.join(DATA_CACHE_DIR, \"prachathai67k\")\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "        print(f\"Unpacking {data_filename}...\")\n",
    "        os.system(f\"unzip {data_filename} -d {data_dir}\")\n",
    "        os.system(f\"mv {data_dir}/data/* {data_dir}\")\n",
    "        os.system(f\"rm -rf {data_dir}/data/\")\n",
    "    else:\n",
    "        print(f\"{data_dir} already exists, skipping unpacking...\")\n",
    "    \n",
    "    shard_filenames = sorted(glob.glob(os.path.join(data_dir, \"*.jsonl\")))\n",
    "    data = []\n",
    "    with open(shard_filenames[0], \"r\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "            \n",
    "    print(\"Download done.\")\n",
    "    print(f\"Number of shards: {len(shard_filenames)}\")\n",
    "    print(f\"Example story:\\n{data[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "49b7081e-d482-4038-b4ee-1b022a294ef8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare():\n",
    "    '''\n",
    "    combine train.jsonl and valid.jsonl and convert all three jsonl to \n",
    "    \n",
    "    <title> <bodytext> <topic> ...\n",
    "    '''\n",
    "    data_dir = os.path.join(DATA_CACHE_DIR, \"prachathai67k\")\n",
    "    shard_filenames = sorted(glob.glob(os.path.join(data_dir, \"*.jsonl\")))\n",
    "    \n",
    "    txt_filename = os.path.join(data_dir, 'train.txt')\n",
    "    \n",
    "    \n",
    "    train = []\n",
    "    test = []\n",
    "    \n",
    "    if not os.path.exists(txt_filename):\n",
    "\n",
    "        for shard in shard_filenames:\n",
    "            if 'train' in shard or 'valid' in shard:\n",
    "                with open(shard, \"r\") as f:\n",
    "                    for line in f:\n",
    "                        data = json.loads(line)\n",
    "                        text = [v for k,v in data.items() if k in ['title', 'body_text']]\n",
    "                        text = [i.replace('\\n', ' ') for i in text]\n",
    "                        text.extend([k for k, v in data.items() if v == 1])\n",
    "                        text = ' '.join(text)\n",
    "                        text.replace('\\n', ' ')\n",
    "                        train.append(text)\n",
    "            else:\n",
    "                with open(shard, \"r\") as f:\n",
    "                    for line in f:\n",
    "                        data = json.loads(line)\n",
    "                        text = [v for k,v in data.items() if k in ['title', 'body_text']]\n",
    "                        text = [i.replace('\\n', ' ') for i in text]\n",
    "                        text.extend([k for k, v in data.items() if v == 1])\n",
    "                        text = ' '.join(text)\n",
    "                        text.replace('\\n', ' ')\n",
    "                        test.append(text)\n",
    "\n",
    "        train_txt_filename = os.path.join(data_dir, 'train.txt')\n",
    "        test_txt_filename  = os.path.join(data_dir, 'test.txt')\n",
    "        with open(train_txt_filename, 'w') as f:\n",
    "            for i in train:\n",
    "                f.write(i + '\\n')\n",
    "\n",
    "        with open(test_txt_filename, 'w') as f:\n",
    "            for i in test:\n",
    "                f.write(i + '\\n')\n",
    "    else:\n",
    "        train_txt_filename = os.path.join(data_dir, 'train.txt')\n",
    "        test_txt_filename  = os.path.join(data_dir, 'test.txt')\n",
    "        print(f\"{txt_filename} already exists, skipping preparing...\")\n",
    "    \n",
    "    train = []\n",
    "    test = []\n",
    "    with open(train_txt_filename, 'r') as f:\n",
    "        for i in f:\n",
    "            train.append(i.strip())\n",
    "\n",
    "    with open(test_txt_filename, 'r') as f:\n",
    "        for i in f:\n",
    "            test.append(i.strip())\n",
    "\n",
    "    \n",
    "    print(f\"Number of train samples : {len(train)}\")\n",
    "    print(f\"Number of test samples : {len(test)}\")\n",
    "    \n",
    "    print(f\"Train sample : \\n\\t{train[0]}\\n\")\n",
    "    print(f\"Test sample : \\n\\t{test[0]}\\n\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8e9470bb-08fe-4437-bcb9-33c11f417003",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/data.zip already exists, skipping download...\n",
      "data/prachathai67k already exists, skipping unpacking...\n",
      "Download done.\n",
      "Number of shards: 3\n",
      "Example story:\n",
      "{'url': 'https://prachatai.com/print/62490', 'date': '2015-11-17 18:14', 'title': 'แฮคเกอร์ Anonymous ลั่นทำสงครามไซเบอร์ครั้งใหญ่สุดกับกลุ่ม IS', 'body_text': '17 พ.ย. 2558 Blognone [1] รายงานว่า กลุ่มแฮคเกอร์ Anonymous ประกาศสงครามไซเบอร์กับกลุ่มหัวรุนแรงหลังจากกลุ่ม IS ออกมาประกาศว่าเป็นผู้อยู่เบื้องหลังการโจมตีกรุงปารีสในคืนวันศุกร์ที่ผ่านมา\\n\\n\\nภาพในคลิปใน YouTube โฆษกของกลุ่มแฮคเกอร์สวมหน้ากากที่เป็นสัญลักษณ์ของกลุ่มได้ออกมาอ่านแถลงเป็นภาษาฝรั่งเศส มีใจความว่า จากการโจมตีของกลุ่ม IS ในกรุงปารีส กลุ่ม Anonymous ทั่วโลกจะตามล่ากลุ่ม IS เหมือนที่เคยทำตอนที่มีการโจมตีสำนักพิมพ์ Charlie Hebdo และครั้งนี้จะเป็นปฏิบัติการโจมตีครั้งใหญ่ที่สุดของกลุ่ม Anonymous เลย นอกจากนี้กลุ่ม Anonymous ยังแสดงความเสียใจต่อครอบครัวผู้สูญเสียในเหตุการณ์ครั้งนี้\\nกลุ่ม Anonymous เคยประกาศสงครามกับกลุ่ม IS หลังจากการโจมตีสำนักพิมพ์ Charlie Hebdo ที่ฝรั่งเศสเมื่อต้นปีที่ผ่านมา ซึ่งครั้งนั้นกลุ่ม Anonymous อ้างว่าได้ระงับบัญชีผู้ใช้งานที่เกี่ยวข้องกับ IS ไปหลายพันบัญชี (อ่านรายละเอียดเพิ่มเติม จากBlognone ที่\\xa0\\xa0กลุ่มแฮคเกอร์ Anonymous ประกาศสงครามไซเบอร์ขอกวาดล้างพวก ISIS [2])', 'politics': 0, 'human_rights': 0, 'quality_of_life': 0, 'international': 1, 'social': 0, 'environment': 0, 'economics': 0, 'culture': 0, 'labor': 0, 'national_security': 0, 'ict': 1, 'education': 0}\n"
     ]
    }
   ],
   "source": [
    "download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3567b65f-b922-4140-91e1-0622fe7c914b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples : 61100\n",
      "Number of test samples : 6789\n",
      "Train sample : \n",
      "\tวุฒิสภาจี้หาทางออกเหมืองโปแตช ประชาไท9 ก.พ. 2549 เมื่อวันที่ 8 ก.พ. เวลาประมาณ 14.00 น. คณะกรรมาธิการสิ่งแวดล้อมวุฒิสภาได้จัดเวทีประชุมติดตามความคืบหน้าโครงการเหมืองแร่โปแตช จ. อุดรธานี เนื่องจากเห็นว่ามีการผลักดันโครงการอย่างเร่งด่วนโดยไม่คำนึงถึงการมีส่วนร่วมของประชาชนในพื้นที่ ณ ห้องประชุมกรรมาธิการหมายเลข 306 อาคารรัฐสภาตามข้อเรียกร้องของกลุ่มอนุรักษ์สิ่งแวดล้อมอุดรธานี โดยได้เชิญหน่วยงานที่เกี่ยวข้องได้แก่ กรมอุตสาหกรรมพื้นฐานและการเหมืองแร่ สำนักงานนโยบายและแผนทรัพยากรธรรมชาติสิ่งแวดล้อม กลุ่มอนุรักษ์สิ่งแวดล้อมอุดรธานี เข้ามาชี้แจงให้ข้อมูล      นายสุรพงษ์ เชียงทอง เจ้าหน้าที่กรมอุตสาหกรรมพื้นฐานและการเหมืองแร่ (กพร.) รายงานต่อที่ประชุมว่าขณะนี้บริษัทได้ยื่นขอประธานบัตรทำเหมืองแล้วอยู่ระหว่างการขึ้นรูปแผนที่พื้นที่ทำเหมืองใต้ดิน ในขณะนี้ทางบริษัทต้องทำการรังวัดปักหมดเขตที่ตั้งโรงแยกแร่ หรือเหมืองแร่บนดินและขึ้นรูปแผนที่เพื่อจะได้ติดประกาศในท้องถิ่นก่อนที่อธิบดีกพร.จะรับรอง แต่อย่างไรก็ตามขณะนี้บริษัยยังไม่ได้ยื่นเอกสารเพื่อการพิจารณาเข้ามาอย่างครบถ้วน และยังต้องดำเนินการอีกหลายขั้นตอนตามกฎหมายแร่ฉบับปี 2545        นายแก้วสรร อติโพธิ ประธานกรรมาธิการสิ่งแวดล้อมวุฒิสภา กล่าวว่าจาการที่เจ้าหน้าที่จากสำนักงานนโยบายและแผนทรัพยากรธรรมชาติและสิ่งแวดล้อม(สผ.)รายงานต่อที่ประชุมว่าคณะนี้บริษัทยังไม่ได้ส่งรายงานฉบับใหม่ที่ต้องมีรายละเอียดเรื่องการศึกษาวิเคราะห์ผลกระทบตามสารสำคัญของกฎหมายแร่ฉบับใหม่ที่ครอบคลุมการทำเหมืองแร่ใต้ดินให้ สผ.พิจารณา อย่างไรก็ตามในการพิจารณาของ สผ. ซึ่งเป็นหน่วยงานราชการมีอำนาจจะตั้งคำถามทางเทคนิควิธีการเพื่อสร้างทางเลือก เช่น การทำเหมืองแร่ใต้ดินแบบที่บริษัทเสนอเป็นแบบช่องทางสลับค้ำยันนี้ปลอดภัยจริงหรือไม่เมื่อเหมืองอยู่ใต้ชุมชน มีทางเลือกวิธีการทำเหมืองใต้ดินแบบที่ปลอดภัยกว่านี้เช่นแบบเหมืองละลายแร่ (Solution mining) เรื่อง กองหางแร่ไม่ต้องพิจารณาเพียงว่าจะใช้ผ้ายางหนาเท่าใดหรือมีผู้เชี่ยวชาญหรือไม่เมื่อนำเอาเกลือปริมาณมากกองบนผิวดินมันจะต้องมีผลกระทบแน่ ๆ ทางกรรมาธิการเป็นห่วงเรื่องนี้มาก จะต้องชี้แจงให้บริษัทแก้ไข การนำเกลือลงไปถมกลับนั้นเราก็รู้กันอยู่ว่าเกลือมีราคาและการนำกลับลงไปก็มีค่าใช้จ่าย และทั้ง สผ. และ กพร. ต่างมีความเห็นพ้องกันว่าสามารถนำไปใช้ในอุตสาหกรรมต่อเนื่องได้ บริษัทจะต้องชี้แจงให้ชัดเจนในรายงานว่าเกลือจะขายให้อุตสาหกรรมต่อเนื่องหรือจะถมกลับ        นายแก้วสรร ยังกล่าวอีกว่าแม้บริษัทจะยื่นขอประทานบัตรเป็นบริเวณกว้าง 2 แหล่งมีชุมชนหลายชุมชนตั้งอยู่ข้างบน กพร.ก็ไม่จำเป็นต้องอนุญาตทั้งหมดที่บริษัทขอ มันขึ้นอยู่ที่การพิจารณาของ กพร. จะเป็นไปได้หรือไม่ที่จะละเว้นเขตที่มีชุมชนตั้งอยู่ และทางจังหวัดจะต้องจัดทำแผนที่แสดงความหนาแน่นของประชากรให้ชัดเจนว่าเขตนี้มีประชากรอยู่เท่าไหร่ ถ้ามีประชากรอยู่ก็ละเว้นไม่อนุญาตให้ทำ เพราะหน่วยงานราชการต้องสร้างทางเลือกที่เหมาะสม  ลดความขัดแย้งเพราะถ้าสร้างเหมืองแร่บนความขัดแย้ง  ในชุมชนก็ไม่มีวันสงบ หากชุมชนไม่ยอมรับเหมือนกรณีโครงการท่อก๊าซไทยมาเลเซีย ที่สร้างบนความขัดแย้งปัจจุบันโครงการก็อยู่อย่างหวาดผวาว่าจะมีคนมาวางระเบิดเมื่อไร นายแก้วสรรกล่าว        ด้านพลเอกสมคิด ศรีสังคม สว.อุดรธานี กล่าวว่า สิ่งที่น่าห่วงหากเกิดโครงการคือเรื่องผลกระทบจากเกลือหางแร่เพราะภาคอีสานลมแรงในฤดูแล้ง พายุฤดูแล้งลมแรงมาก ขณะที่ฤดูฝนก็มีฝนมาก ที่ตั้งโครงการก็เช่นกันโรงแต่งแร่ตั้งอยู่บนเนินสูง กองเกลือกว้างเป็นกิโล และสูง 40 เมตรไม่ต้องใช้ผู้เชี่ยวชาญพิจารณาให้ลำบากก็เห็นว่าจะมีผลกระทบอยู่ชัด ๆ ในฐานะคนอุดรธานีผมไม่เห็นด้วยกับโครงการไม่อยากให้สร้างเหมืองแร่ในจังหวัดอุดรธานี        ด้านนางมณี บุญรอด  รองประธานกลุ่มอนุรักษ์สิ่งแวดล้อมอุดรธานี กล่าวชี้แจงว่าปัจจุบันชาวบ้านไม่ยอมรับโครงการและมีการทำงานแย่งแยกชาวบ้าน เกิดความขัดแย้งรุนแรงขึ้นเรื่อย ๆ แม้ยังไม่สร้างโครงการ คือสิ่งที่น่าเป็นห่วงที่สุด และหากโครงการยังดำรงอยู่ความขัดแย้งก็รุนแรงขึ้นเรื่อย ๆ แม้บริษัทจะอ้างว่าได้ลงทุนไปแล้วหลายพันล้าน รวมทั้งบริษัทอ้างว่าจะฟ้องร้องรัฐบาลไทยหากไม่ได้ทำเหมือง และจะเป็นการทำลายความเชื่อมั่นคงนักลงทุนต่างชาติ  นั้นเป็นการพูดแต่ได้เพราะบริษัทเองละเลยขั้นตอนกฎหมายไทย ละเมิดสิทธิคนไทย  และกำลังมีการยุแยงให้คนไทย คนในชุมชนเดียวกันขัดแย้งเข่นฆ่ากันเอง politics\n",
      "\n",
      "Test sample : \n",
      "\tแฮคเกอร์ Anonymous ลั่นทำสงครามไซเบอร์ครั้งใหญ่สุดกับกลุ่ม IS 17 พ.ย. 2558 Blognone [1] รายงานว่า กลุ่มแฮคเกอร์ Anonymous ประกาศสงครามไซเบอร์กับกลุ่มหัวรุนแรงหลังจากกลุ่ม IS ออกมาประกาศว่าเป็นผู้อยู่เบื้องหลังการโจมตีกรุงปารีสในคืนวันศุกร์ที่ผ่านมา   ภาพในคลิปใน YouTube โฆษกของกลุ่มแฮคเกอร์สวมหน้ากากที่เป็นสัญลักษณ์ของกลุ่มได้ออกมาอ่านแถลงเป็นภาษาฝรั่งเศส มีใจความว่า จากการโจมตีของกลุ่ม IS ในกรุงปารีส กลุ่ม Anonymous ทั่วโลกจะตามล่ากลุ่ม IS เหมือนที่เคยทำตอนที่มีการโจมตีสำนักพิมพ์ Charlie Hebdo และครั้งนี้จะเป็นปฏิบัติการโจมตีครั้งใหญ่ที่สุดของกลุ่ม Anonymous เลย นอกจากนี้กลุ่ม Anonymous ยังแสดงความเสียใจต่อครอบครัวผู้สูญเสียในเหตุการณ์ครั้งนี้ กลุ่ม Anonymous เคยประกาศสงครามกับกลุ่ม IS หลังจากการโจมตีสำนักพิมพ์ Charlie Hebdo ที่ฝรั่งเศสเมื่อต้นปีที่ผ่านมา ซึ่งครั้งนั้นกลุ่ม Anonymous อ้างว่าได้ระงับบัญชีผู้ใช้งานที่เกี่ยวข้องกับ IS ไปหลายพันบัญชี (อ่านรายละเอียดเพิ่มเติม จากBlognone ที่  กลุ่มแฮคเกอร์ Anonymous ประกาศสงครามไซเบอร์ขอกวาดล้างพวก ISIS [2]) international ict\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92f733c-e15c-4347-81a3-182e504e2863",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb4c1c0e-4cde-43c6-b3ab-0b6c96ae6553",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from sentencepiece import SentencePieceProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "507fd847-da3b-481b-9c2d-3b4db4a4d3f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TOKENIZER_MODEL = \"tokenizer.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "423df2a1-f264-4204-9c9b-86347aa46d75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, tokenizer_model=None):\n",
    "        model_path = tokenizer_model if tokenizer_model else TOKENIZER_MODEL\n",
    "        assert os.path.isfile(model_path), model_path\n",
    "        self.sp_model = SentencePieceProcessor(model_file=model_path)\n",
    "        self.model_path = model_path\n",
    "\n",
    "        # BOS / EOS token IDs\n",
    "        self.n_words: int = self.sp_model.vocab_size()\n",
    "        self.bos_id: int = self.sp_model.bos_id()\n",
    "        self.eos_id: int = self.sp_model.eos_id()\n",
    "        self.pad_id: int = self.sp_model.pad_id()\n",
    "        #print(f\"#words: {self.n_words} - BOS ID: {self.bos_id} - EOS ID: {self.eos_id}\")\n",
    "        assert self.sp_model.vocab_size() == self.sp_model.get_piece_size()\n",
    "\n",
    "    def encode(self, s: str, bos: bool, eos: bool) -> List[int]:\n",
    "        assert type(s) is str\n",
    "        t = self.sp_model.encode(s)\n",
    "        if bos:\n",
    "            t = [self.bos_id] + t\n",
    "        if eos:\n",
    "            t = t + [self.eos_id]\n",
    "        return t\n",
    "\n",
    "    def decode(self, t: List[int]) -> str:\n",
    "        return self.sp_model.decode(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f17ceb9c-c7bc-425e-b039-eb9c391c34a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encode : [1, 29871, 31070, 30289, 30727, 227, 188, 134, 30348, 30759, 30496, 30507, 31010, 227, 188, 134, 30348, 2]\n",
      "Decode : ภาพในคลิปใน\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "print(f\"Encode : {t.encode('ภาพในคลิปใน', 1, 1)}\")\n",
    "print(f\"Decode : {t.decode(t.encode('ภาพในคลิปใน',1, 1))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7c0b5a-378c-4697-95d0-ada78c713639",
   "metadata": {},
   "source": [
    "### Tokenize the dataset using llama2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a21e26e-5b3d-423a-aa91-0d49cf9cee2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bd4cdd48-6f27-46fc-8670-809300154ade",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pretokenize():\n",
    "    data_dir = os.path.join(DATA_CACHE_DIR, \"prachathai67k\")\n",
    "    enc = Tokenizer()\n",
    "    \n",
    "    filenames = sorted(glob.glob(os.path.join(data_dir, \"*.txt\")))\n",
    "    \n",
    "    \n",
    "    for filename in filenames:\n",
    "        txt_basename = os.path.basename(filename)\n",
    "        bin_basename = txt_basename.replace('.txt', '.bin')\n",
    "        tokenized_filename = os.path.join(data_dir, bin_basename)\n",
    "        \n",
    "        if not os.path.exists(tokenized_filename):\n",
    "        \n",
    "            all_tokens = []\n",
    "            with open(filename, \"r\") as f:\n",
    "                for text in f:\n",
    "                    text = text.strip()\n",
    "                    if text:\n",
    "                        tokens = enc.encode(text, bos=True, eos=True)\n",
    "                        all_tokens.extend(tokens)\n",
    "\n",
    "            all_tokens = np.array(all_tokens, dtype=np.uint16)\n",
    "\n",
    "\n",
    "\n",
    "            with open(tokenized_filename, \"wb\") as f:\n",
    "                f.write(all_tokens.tobytes())\n",
    "\n",
    "            avg_seq_len = all_tokens.size / ((all_tokens == 1).sum())\n",
    "            print(f\"Saved {tokenized_filename}, average seqlen: {avg_seq_len:.4f}\")\n",
    "        else:\n",
    "            print(f\"{tokenized_filename} exists,  pretokenization is already done for {txt_basename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3a4afd5d-c0d9-4a92-a826-65b20a44be56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/prachathai67k/test.bin exists,  pretokenization is already done for test.txt\n",
      "data/prachathai67k/train.bin exists,  pretokenization is already done for train.txt\n"
     ]
    }
   ],
   "source": [
    "pretokenize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c15021-eac7-48e8-8f20-21c83046d0fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Batch Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe3e58f8-b81b-479b-ae63-01b35558f3b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2d15b6b-c662-4f41-bfe1-bbde20a973f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PretokDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, split, max_seq_len, vocab_size):\n",
    "        super().__init__()\n",
    "        self.split = split\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        worker_id = worker_info.id if worker_info else 0\n",
    "        rank = dist.get_rank() if dist.is_initialized() else 0\n",
    "        seed = 42 + worker_id + 1337 * rank\n",
    "        rng = random.Random(seed)\n",
    "        print(f\"Created a PretokDataset with rng seed {seed}\")\n",
    "        \n",
    "        bin_dir = os.path.join(DATA_CACHE_DIR, \"prachathai67k\")\n",
    "        filename = os.path.join(bin_dir, \"train.bin\") if self.split == \"train\" else os.path.join(bin_dir, \"test.bin\")\n",
    "        \n",
    "        assert len(filename)>0, f\"No bin files found in {bin_dir}\"\n",
    "        \n",
    "        \n",
    "        while True:\n",
    "                \n",
    "            m = np.memmap(filename, dtype=np.uint16, mode=\"r\")\n",
    "            num_batches = len(m) // self.max_seq_len\n",
    "            num_batches -= 1\n",
    "            assert num_batches > 0, \"this file is way too small? investigatte.\"\n",
    "            ixs = list(range(num_batches))\n",
    "            rng.shuffle(ixs)\n",
    "            for ix in ixs:\n",
    "                start = ix * self.max_seq_len\n",
    "                end = start + self.max_seq_len + 1\n",
    "                chunk = torch.from_numpy((m[start: end]).astype(np.int64))\n",
    "                x = chunk[:-1]\n",
    "                y = chunk[1:]\n",
    "                yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a4bb4a5-2457-4bc9-9b7c-634621d139bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Task:\n",
    "    \n",
    "    @staticmethod\n",
    "    def iter_batches(batch_size, device, num_workers=0, **dataset_kwargs):\n",
    "        ds = PretokDataset(**dataset_kwargs)\n",
    "        dl = torch.utils.data.DataLoader(\n",
    "            ds, batch_size=batch_size, pin_memory=False, num_workers=num_workers\n",
    "        )\n",
    "        for x, y in dl:\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9563499-680c-4320-9a6a-90d7d0e2e815",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "max_seq_len = 128\n",
    "vocab_size = 32000\n",
    "split=\"Train\"\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "479130c8-0f3f-4f05-ad75-954061d122d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iter_batches = partial(\n",
    "    Task.iter_batches,\n",
    "    batch_size= batch_size,\n",
    "    split=split,\n",
    "    max_seq_len =max_seq_len,\n",
    "    vocab_size = vocab_size,\n",
    "    device = device,\n",
    "    num_workers=0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c742c30-9603-47b8-b801-f3f5a3ca5182",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_iter = iter_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23bbe001-268c-4dbc-a0f5-b68657f2b01b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[30297, 30501, 30425, 30289],\n",
      "        [29953, 29889, 29900, 29900],\n",
      "        [30401, 30618, 31422, 30351],\n",
      "        [30398, 30398, 30289, 30348]])\n",
      "tensor([[30501, 30425, 30289, 30297],\n",
      "        [29889, 29900, 29900, 29871],\n",
      "        [30618, 31422, 30351, 30348],\n",
      "        [30398, 30289, 30348, 30297]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 128]), torch.Size([8, 128]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(batch_iter)\n",
    "print(X[:4, :4])\n",
    "print(y[:4, :4])\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceede1db-d22d-470c-9f0e-8b9a4a5a1ee6",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559d8d1c-2dfe-4964-8a1c-5af432ca7f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LC_ALL=\"en_US.UTF-8\"\n",
    "!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
    "!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
    "!ldconfig /usr/lib64-nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b278423a-b36d-4eba-949d-4f45c70c0862",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "from contextlib import nullcontext\n",
    "\n",
    "from model import Transformer, ModelArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "396747da-8cba-46fa-8688-b4bb0b0763d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# I/O\n",
    "out_dir = \"out\" \n",
    "eval_interval = 20\n",
    "log_interval = 1\n",
    "eval_iters = 10\n",
    "eval_only = False  # if True, script exits right after the first eval\n",
    "always_save_checkpoint = False  # if True, always save a checkpoint after each eval\n",
    "init_from = \"scratch\"  # 'scratch' or 'resume'\n",
    "# wandb logging\n",
    "wandb_log = False  # disabled by default\n",
    "wandb_project = \"llamac\"\n",
    "wandb_run_name = \"run\" + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "# data\n",
    "batch_size = 8  # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "max_seq_len = 256\n",
    "vocab_source = \"llama2\" # llama2|custom; use Lllama 2 vocab from Meta, or custom trained\n",
    "vocab_size = 32000 # the Llama 2 tokenizer has 32K tokens\n",
    "# model\n",
    "dim = 288\n",
    "n_layers = 6\n",
    "n_heads = 6\n",
    "n_kv_heads = 6\n",
    "multiple_of = 32\n",
    "dropout = 0.0\n",
    "# adamw optimizer\n",
    "gradient_accumulation_steps = 4  # used to simulate larger batch sizes\n",
    "learning_rate = 5e-4  # max learning rate\n",
    "max_iters = 100000  # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0  # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True  # whether to decay the learning rate\n",
    "warmup_iters = 1000  # how many steps to warm up for\n",
    "# system\n",
    "device = \"cpu\"  # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "dtype = \"bfloat16\"  # float32|bfloat16|float16\n",
    "compile = True  # use PyTorch 2.0 to compile the model to be faster\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e32f20cf-2a8d-4a22-bf6a-2a7b5be3caf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fixing some hyperparams to sensible defaults\n",
    "lr_decay_iters = max_iters  # should be ~= max_iters per Chinchilla\n",
    "min_lr = 0.0  # minimum learning rate, should be ~= learning_rate/10 per Chinchilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4611ac0d-e178-4cd7-b0c8-5d20ffb671a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "master_process = True\n",
    "seed_offset = 0\n",
    "ddp_world_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48bdecd3-c850-4fc4-890a-c42d09e44929",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens per iteration will be: 8,192\n",
      "breaks down as: 4 grad accum steps * 1 processes * 8 batch_size * 256 max seq len\n"
     ]
    }
   ],
   "source": [
    "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * max_seq_len\n",
    "\n",
    "if master_process:\n",
    "    print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
    "    print(f\"breaks down as: {gradient_accumulation_steps} grad accum steps * {ddp_world_size} processes * {batch_size} batch_size * {max_seq_len} max seq len\")\n",
    "    \n",
    "if master_process:\n",
    "    os.makedirs(out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a086058-11ba-4205-8a8b-127d57a82310",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "device_type = \"cuda\" if \"cuda\" in device else \"cpu\"\n",
    "\n",
    "ptdtype = {\"float32\": torch.float32, \"bfloat16\": torch.bfloat16, \"float16\": torch.float16}[dtype]\n",
    "\n",
    "ctx = (\n",
    "    nullcontext()\n",
    "    if device_type == \"cpu\"\n",
    "    else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2308969c-d4c1-4a25-8832-eb1f47da0b13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# task-specific setup\n",
    "iter_batches = partial(\n",
    "    Task.iter_batches,\n",
    "    batch_size=batch_size,\n",
    "    max_seq_len=max_seq_len,\n",
    "    vocab_size=vocab_size,\n",
    "    device=device,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5804b811-d369-4c91-9fe5-7085f1c4bc61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing a new model from scratch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (tok_embeddings): Embedding(32000, 288)\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x TransformerBlock(\n",
       "      (attention): Attention(\n",
       "        (wq): Linear(in_features=288, out_features=288, bias=False)\n",
       "        (wk): Linear(in_features=288, out_features=288, bias=False)\n",
       "        (wv): Linear(in_features=288, out_features=288, bias=False)\n",
       "        (wo): Linear(in_features=288, out_features=288, bias=False)\n",
       "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (w1): Linear(in_features=288, out_features=768, bias=False)\n",
       "        (w2): Linear(in_features=768, out_features=288, bias=False)\n",
       "        (w3): Linear(in_features=288, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (attention_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=288, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "model_args = dict(\n",
    "    dim=dim,\n",
    "    n_layers=n_layers,\n",
    "    n_heads=n_heads,\n",
    "    n_kv_heads=n_kv_heads,\n",
    "    vocab_size=vocab_size,\n",
    "    multiple_of=multiple_of,\n",
    "    max_seq_len=max_seq_len,\n",
    "    dropout=dropout,\n",
    ")\n",
    "\n",
    "if init_from == \"scratch\":\n",
    "    \n",
    "    print(\"Initializing a new model from scratch\")\n",
    "    gptconf = ModelArgs(**model_args)\n",
    "    model = Transformer(gptconf)\n",
    "    \n",
    "elif init_from == \"resume\":\n",
    "    print(f\"Resuming training from {out_dir}\")\n",
    "    # resume training from a checkpoint.\n",
    "    ckpt_path = os.path.join(out_dir, \"ckpt.pt\")\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    checkpoint_model_args = checkpoint[\"model_args\"]\n",
    "    # force these config attributes to be equal otherwise we can't even resume training\n",
    "    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
    "    for k in [\"dim\", \"n_layers\", \"n_heads\", \"n_kv_heads\", \"vocab_size\", \"multiple_of\", \"max_seq_len\"]:\n",
    "        model_args[k] = checkpoint_model_args[k]\n",
    "    # create the model\n",
    "    gptconf = ModelArgs(**model_args)\n",
    "    model = Transformer(gptconf)\n",
    "    state_dict = checkpoint[\"model\"]\n",
    "    # fix the keys of the state dictionary :(\n",
    "    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
    "    unwanted_prefix = \"_orig_mod.\"\n",
    "    for k, v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "    iter_num = checkpoint[\"iter_num\"]\n",
    "    best_val_loss = checkpoint[\"best_val_loss\"]\n",
    "model.to(device)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8aeeb909-ec8c-42af-bf0a-53d51cbf0149",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+------------+\n",
      "|             Modules             | Parameters |\n",
      "+---------------------------------+------------+\n",
      "|      tok_embeddings.weight      |  9216000   |\n",
      "|   layers.0.attention.wq.weight  |   82944    |\n",
      "|   layers.0.attention.wk.weight  |   82944    |\n",
      "|   layers.0.attention.wv.weight  |   82944    |\n",
      "|   layers.0.attention.wo.weight  |   82944    |\n",
      "| layers.0.feed_forward.w1.weight |   221184   |\n",
      "| layers.0.feed_forward.w2.weight |   221184   |\n",
      "| layers.0.feed_forward.w3.weight |   221184   |\n",
      "|  layers.0.attention_norm.weight |    288     |\n",
      "|     layers.0.ffn_norm.weight    |    288     |\n",
      "|   layers.1.attention.wq.weight  |   82944    |\n",
      "|   layers.1.attention.wk.weight  |   82944    |\n",
      "|   layers.1.attention.wv.weight  |   82944    |\n",
      "|   layers.1.attention.wo.weight  |   82944    |\n",
      "| layers.1.feed_forward.w1.weight |   221184   |\n",
      "| layers.1.feed_forward.w2.weight |   221184   |\n",
      "| layers.1.feed_forward.w3.weight |   221184   |\n",
      "|  layers.1.attention_norm.weight |    288     |\n",
      "|     layers.1.ffn_norm.weight    |    288     |\n",
      "|   layers.2.attention.wq.weight  |   82944    |\n",
      "|   layers.2.attention.wk.weight  |   82944    |\n",
      "|   layers.2.attention.wv.weight  |   82944    |\n",
      "|   layers.2.attention.wo.weight  |   82944    |\n",
      "| layers.2.feed_forward.w1.weight |   221184   |\n",
      "| layers.2.feed_forward.w2.weight |   221184   |\n",
      "| layers.2.feed_forward.w3.weight |   221184   |\n",
      "|  layers.2.attention_norm.weight |    288     |\n",
      "|     layers.2.ffn_norm.weight    |    288     |\n",
      "|   layers.3.attention.wq.weight  |   82944    |\n",
      "|   layers.3.attention.wk.weight  |   82944    |\n",
      "|   layers.3.attention.wv.weight  |   82944    |\n",
      "|   layers.3.attention.wo.weight  |   82944    |\n",
      "| layers.3.feed_forward.w1.weight |   221184   |\n",
      "| layers.3.feed_forward.w2.weight |   221184   |\n",
      "| layers.3.feed_forward.w3.weight |   221184   |\n",
      "|  layers.3.attention_norm.weight |    288     |\n",
      "|     layers.3.ffn_norm.weight    |    288     |\n",
      "|   layers.4.attention.wq.weight  |   82944    |\n",
      "|   layers.4.attention.wk.weight  |   82944    |\n",
      "|   layers.4.attention.wv.weight  |   82944    |\n",
      "|   layers.4.attention.wo.weight  |   82944    |\n",
      "| layers.4.feed_forward.w1.weight |   221184   |\n",
      "| layers.4.feed_forward.w2.weight |   221184   |\n",
      "| layers.4.feed_forward.w3.weight |   221184   |\n",
      "|  layers.4.attention_norm.weight |    288     |\n",
      "|     layers.4.ffn_norm.weight    |    288     |\n",
      "|   layers.5.attention.wq.weight  |   82944    |\n",
      "|   layers.5.attention.wk.weight  |   82944    |\n",
      "|   layers.5.attention.wv.weight  |   82944    |\n",
      "|   layers.5.attention.wo.weight  |   82944    |\n",
      "| layers.5.feed_forward.w1.weight |   221184   |\n",
      "| layers.5.feed_forward.w2.weight |   221184   |\n",
      "| layers.5.feed_forward.w3.weight |   221184   |\n",
      "|  layers.5.attention_norm.weight |    288     |\n",
      "|     layers.5.ffn_norm.weight    |    288     |\n",
      "|           norm.weight           |    288     |\n",
      "+---------------------------------+------------+\n",
      "Total Trainable Params: 15191712\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15191712"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ffdfbac5-a971-428a-acab-f7d6ac8078ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == \"float16\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e526e82-d101-4647-a22a-6d00eee9c89c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 43, with 15,187,968 parameters\n",
      "num non-decayed parameter tensors: 13, with 3,744 parameters\n",
      "using fused AdamW: False\n"
     ]
    }
   ],
   "source": [
    "# optimizer\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "if init_from == \"resume\" and \"optimizer\" in checkpoint:\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "checkpoint = None  # free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2b86ca7c-e199-42fe-b4c9-c47d7759e623",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling the model... (takes a ~minute)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkProcess-5:\n",
      "Process ForkProcess-2:\n",
      "Process ForkProcess-8:\n",
      "Process ForkProcess-7:\n",
      "Process ForkProcess-1:\n",
      "Process ForkProcess-6:\n",
      "Process ForkProcess-4:\n",
      "Process ForkProcess-3:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/shane/mambaforge/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n"
     ]
    }
   ],
   "source": [
    "# compile the model\n",
    "if compile:\n",
    "    print(\"compiling the model... (takes a ~minute)\")\n",
    "    unoptimized_model = model\n",
    "    model = torch.compile(model)  # requires PyTorch 2.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ac89681-618c-43df-9a1b-136cd2a15f50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        batch_iter = iter_batches(split=split)\n",
    "        losses = torch.zeros(eval_iters)  # keep on CPU\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = next(batch_iter)\n",
    "            with ctx:\n",
    "                logits = model(X, Y)\n",
    "                loss = raw_model.last_loss\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "51d7f9f1-ef76-493a-a14b-08040b7c117f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a PretokDataset with rng seed 42\n",
      "Created a PretokDataset with rng seed 42\n",
      "Created a PretokDataset with rng seed 42\n",
      "step 0: train loss 10.4505, val loss 10.4505\n",
      "0 | loss 10.4358 | lr 0.000000e+00 | 57994.96ms | mfu -100.00%\n",
      "1 | loss 10.4468 | lr 5.000000e-07 | 7343.64ms | mfu -100.00%\n",
      "2 | loss 10.4694 | lr 1.000000e-06 | 7063.61ms | mfu -100.00%\n",
      "3 | loss 10.4482 | lr 1.500000e-06 | 7257.32ms | mfu -100.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m micro_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(gradient_accumulation_steps):\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[0;32m---> 56\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m         loss \u001b[38;5;241m=\u001b[39m raw_model\u001b[38;5;241m.\u001b[39mlast_loss\n\u001b[1;32m     58\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m gradient_accumulation_steps\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m dynamic_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/llama2-train-from-scratch/model.py:249\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, tokens, targets)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, nn\u001b[38;5;241m.\u001b[39mEmbedding):\n\u001b[1;32m    247\u001b[0m         torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mnormal_(module\u001b[38;5;241m.\u001b[39mweight, mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, std\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.02\u001b[39m)\n\u001b[0;32m--> 249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens: torch\u001b[38;5;241m.\u001b[39mTensor, targets: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    250\u001b[0m     _bsz, seqlen \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    251\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtok_embeddings(tokens)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m dynamic_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/_dynamo/external_utils.py:17\u001b[0m, in \u001b[0;36mwrap_inline.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:3905\u001b[0m, in \u001b[0;36maot_module_simplified.<locals>.forward\u001b[0;34m(*runtime_args)\u001b[0m\n\u001b[1;32m   3903\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(params_flat)\n\u001b[1;32m   3904\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(runtime_args)\n\u001b[0;32m-> 3905\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1482\u001b[0m, in \u001b[0;36mmake_boxed_func.<locals>.g\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mg\u001b[39m(args):\n\u001b[0;32m-> 1482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:2527\u001b[0m, in \u001b[0;36mcreate_runtime_wrapper.<locals>.runtime_wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   2525\u001b[0m             args_[idx] \u001b[38;5;241m=\u001b[39m args_[idx]\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39m_force_original_view_tracking(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m-> 2527\u001b[0m         all_outs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2528\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompiled_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2529\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2530\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2531\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2533\u001b[0m     all_outs \u001b[38;5;241m=\u001b[39m call_func_with_args(\n\u001b[1;32m   2534\u001b[0m         compiled_fn,\n\u001b[1;32m   2535\u001b[0m         args,\n\u001b[1;32m   2536\u001b[0m         disable_amp\u001b[38;5;241m=\u001b[39mdisable_amp,\n\u001b[1;32m   2537\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1506\u001b[0m, in \u001b[0;36mcall_func_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   1505\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1506\u001b[0m         out \u001b[38;5;241m=\u001b[39m normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1507\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1511\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt take boxed arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1512\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1513\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1514\u001b[0m         )\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1482\u001b[0m, in \u001b[0;36mmake_boxed_func.<locals>.g\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mg\u001b[39m(args):\n\u001b[0;32m-> 1482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:3010\u001b[0m, in \u001b[0;36maot_dispatch_autograd.<locals>.CompiledFunction.forward\u001b[0;34m(ctx, *deduped_flat_tensor_args)\u001b[0m\n\u001b[1;32m   3004\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39margs, seed, offset)\n\u001b[1;32m   3005\u001b[0m \u001b[38;5;66;03m# There is a pretty complicated calling convention around what the compiled fw returns.\u001b[39;00m\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;66;03m# The full list of outputs and their relative order is:\u001b[39;00m\n\u001b[1;32m   3007\u001b[0m \u001b[38;5;66;03m# (*mutated_inputs, *fw_outs, *fw_intermediate_bases, *saved_tensors, *saved_symints)\u001b[39;00m\n\u001b[1;32m   3008\u001b[0m \u001b[38;5;66;03m# - Note that in the synthetic bases case, mutated_inputs will correspond to an updated version\u001b[39;00m\n\u001b[1;32m   3009\u001b[0m \u001b[38;5;66;03m#   of the original view, and not the synthetic base\u001b[39;00m\n\u001b[0;32m-> 3010\u001b[0m fw_outs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCompiledFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompiled_fw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3012\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3014\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3016\u001b[0m num_outputs \u001b[38;5;241m=\u001b[39m CompiledFunction\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mnum_outputs\n\u001b[1;32m   3017\u001b[0m num_outputs_aliased \u001b[38;5;241m=\u001b[39m CompiledFunction\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mnum_outputs_aliased\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1506\u001b[0m, in \u001b[0;36mcall_func_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   1505\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1506\u001b[0m         out \u001b[38;5;241m=\u001b[39m normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1507\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1511\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt take boxed arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1512\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1513\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1514\u001b[0m         )\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/_inductor/codecache.py:374\u001b[0m, in \u001b[0;36mCompiledFxGraph.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_current_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/_inductor/codecache.py:401\u001b[0m, in \u001b[0;36m_run_from_cache\u001b[0;34m(compiled_graph, inputs)\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcodecache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyCodeCache\n\u001b[1;32m    393\u001b[0m     compiled_graph\u001b[38;5;241m.\u001b[39mcompiled_artifact \u001b[38;5;241m=\u001b[39m PyCodeCache\u001b[38;5;241m.\u001b[39mload_by_key_path(\n\u001b[1;32m    394\u001b[0m         compiled_graph\u001b[38;5;241m.\u001b[39mcache_key,\n\u001b[1;32m    395\u001b[0m         compiled_graph\u001b[38;5;241m.\u001b[39martifact_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m (),\n\u001b[1;32m    399\u001b[0m     )\u001b[38;5;241m.\u001b[39mcall\n\u001b[0;32m--> 401\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompiled_artifact\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/torchinductor_shane/xj/cxjpjiq3w2gzaigab6jpmvazx24lrx4f4vdclxsal2oa4zv56qjm.py:2004\u001b[0m, in \u001b[0;36mcall\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m   2002\u001b[0m buf175 \u001b[38;5;241m=\u001b[39m empty_strided((\u001b[38;5;241m2048\u001b[39m, \u001b[38;5;241m32000\u001b[39m), (\u001b[38;5;241m32000\u001b[39m, \u001b[38;5;241m1\u001b[39m), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# Source Nodes: [l__self___output], Original ATen: [aten.mm]\u001b[39;00m\n\u001b[0;32m-> 2004\u001b[0m \u001b[43mextern_kernels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf174\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreinterpret_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprimals_57\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m288\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m288\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuf175\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2005\u001b[0m buf176 \u001b[38;5;241m=\u001b[39m empty_strided((\u001b[38;5;241m2048\u001b[39m, \u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2048\u001b[39m), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m   2006\u001b[0m buf177 \u001b[38;5;241m=\u001b[39m empty_strided((\u001b[38;5;241m2048\u001b[39m, \u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2048\u001b[39m), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "train_batch_iter = iter_batches(split=\"train\")\n",
    "X, Y = next(train_batch_iter) # fetch the very first batch\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "raw_model = model\n",
    "\n",
    "running_mfu = -1.0\n",
    "\n",
    "while True:\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "        \n",
    "    if iter_num % eval_interval == 0 and master_process:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if wandb_log:\n",
    "            try:\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"iter\": iter_num,\n",
    "                        \"tokens\": iter_num * tokens_per_iter,\n",
    "                        \"loss/train\": losses[\"train\"],\n",
    "                        \"loss/val\": losses[\"val\"],\n",
    "                        \"lr\": lr,\n",
    "                        \"mfu\": running_mfu * 100,  # convert to percentage\n",
    "                    }, step = iter_num\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"logging to wandb failed: {e}\")\n",
    "        if losses[\"val\"] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses[\"val\"]\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    \"model\": raw_model.state_dict(),\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                    \"model_args\": model_args,\n",
    "                    \"iter_num\": iter_num,\n",
    "                    \"best_val_loss\": best_val_loss,\n",
    "                    \n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, \"ckpt.pt\"))\n",
    "                \n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        \n",
    "        with ctx:\n",
    "            logits = model(X, Y)\n",
    "            loss = raw_model.last_loss\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        X, Y = next(train_batch_iter)\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0 and master_process:\n",
    "        # get loss as float, scale up due to the divide above. note: this is a CPU-GPU sync point\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5:  # let the training loop settle a bit\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9 * running_mfu + 0.1 * mfu\n",
    "        print(\n",
    "            f\"{iter_num} | loss {lossf:.4f} | lr {lr:e} | {dt*1000:.2f}ms | mfu {running_mfu*100:.2f}%\"\n",
    "        )\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90db2a65-46cf-41a0-9e80-1fb786785e42",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd3c347-f125-4e1a-9d91-abe33256dab0",
   "metadata": {},
   "source": [
    "### Load model to torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3be032b5-c0e3-4439-b033-ec2916ade7f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "from contextlib import nullcontext\n",
    "from functools import partial\n",
    "\n",
    "from model import Transformer, ModelArgs\n",
    "from tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2b77373-cf56-46db-9901-7b8e5c02d13d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_dir = 'out_thai'\n",
    "device = 'cpu'\n",
    "device_type = \"cuda\" if \"cuda\" in device else \"cpu\"\n",
    "\n",
    "# model\n",
    "dim = 288\n",
    "n_layers = 6\n",
    "n_heads = 6\n",
    "n_kv_heads = 6\n",
    "multiple_of = 32\n",
    "dropout = 0.0\n",
    "\n",
    "\n",
    "vocab_size = 32000\n",
    "max_seq_len = 512\n",
    "\n",
    "model_args = dict(\n",
    "    dim=dim,\n",
    "    n_layers=n_layers,\n",
    "    n_heads=n_heads,\n",
    "    n_kv_heads=n_kv_heads,\n",
    "    vocab_size=vocab_size,\n",
    "    multiple_of=multiple_of,\n",
    "    max_seq_len=max_seq_len,\n",
    "    dropout=dropout,\n",
    ")\n",
    "\n",
    "\n",
    "ctx = (\n",
    "    nullcontext()\n",
    "    if device_type == \"cpu\"\n",
    "    else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "659ebb2f-7ce0-40a5-97b7-c58340eb1318",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling the model... (takes a ~minute)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): Transformer(\n",
       "    (tok_embeddings): Embedding(32000, 288)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=288, out_features=288, bias=False)\n",
       "          (wk): Linear(in_features=288, out_features=288, bias=False)\n",
       "          (wv): Linear(in_features=288, out_features=288, bias=False)\n",
       "          (wo): Linear(in_features=288, out_features=288, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=288, out_features=768, bias=False)\n",
       "          (w2): Linear(in_features=768, out_features=288, bias=False)\n",
       "          (w3): Linear(in_features=288, out_features=768, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "    (output): Linear(in_features=288, out_features=32000, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_path = os.path.join(out_dir, \"ckpt.pt\")\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "checkpoint_model_args = checkpoint[\"model_args\"]\n",
    "# force these config attributes to be equal otherwise we can't even resume training\n",
    "# the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
    "for k in [\"dim\", \"n_layers\", \"n_heads\", \"n_kv_heads\", \"vocab_size\", \"multiple_of\", \"max_seq_len\"]:\n",
    "    model_args[k] = checkpoint_model_args[k]\n",
    "# create the model\n",
    "gptconf = ModelArgs(**model_args)\n",
    "model = Transformer(gptconf)\n",
    "state_dict = checkpoint[\"model\"]\n",
    "\n",
    "# fix the keys of the state dictionary :(\n",
    "# honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
    "unwanted_prefix = \"_orig_mod.\"\n",
    "for k, v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "# compile the model\n",
    "if compile:\n",
    "    print(\"compiling the model... (takes a ~minute)\")\n",
    "    unoptimized_model = model\n",
    "    model = torch.compile(model)  # requires PyTorch 2.0\n",
    "\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d44731b-e235-402d-9db8-03e5de525bc3",
   "metadata": {},
   "source": [
    "### Generate text in llama2.c bin format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7114478f-bb8c-4d21-bc49-b9e27dd3b958",
   "metadata": {},
   "source": [
    "#### convert to llama2.c format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6784b39-2f7e-48ca-b0af-4980c066c706",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from export import model_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a972eec9-7bc5-46a3-bbcb-e0ab037d5d64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote out_thai/model.bin\n"
     ]
    }
   ],
   "source": [
    "model_export(model, os.path.join(out_dir, \"model.bin\"), version=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0786213d-3744-408c-ab89-ae4e767af9f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "895501ae-2686-41d2-b36f-8c62baac4425",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "รูปล้อมกรอบอำนาจพระราชดำรัส หลัง รบ.พ.ร..จำลอง กสทช.ยันยังเปิดแลงด้วยวาจาห้ย้ายศาลทหาร ย้ายศาลรัธรรมนูญออกจากราชการ นายสนธิ ยังได้ชี้แจงนรายละเอียดของรูปล้อเลียนศาลยุติธรรม และทำห้การชุมนุมทางการเมืองไม่เป็นไปโดยสงบ ศาลพิจารณาตัดสินแล้วเ\n",
      "achieved tok/s: 73.678128\n",
      "นักข่าวพลเมือง: ประท้วงกลายเป็นส่วนหน่งของการปิวัติสยามประเทศไทยที่นิสิตเสนอไปแล้ว กลุ่มชนชั้นนำของเชียงหม่ได้ยดกุมความับ้อนและลิดรอนกิจกรรมเกี่ยวกับการสร้างสันติสุข มีความิดพลาดอย่างน้อย 7 ปี ก่อนการรัประหาร 4 ครั้งเริ่มจากรับา\n",
      "achieved tok/s: 73.107798\n",
      "เยคนงานหวั่น \"คนหนุ่มที่หวังอีกวัน- รายงานสานการณ์น้ำท่วม แต่ลูกจ้างหวังเดียวกับแรงงานที่รอการพิพากษา ที่โรงงานของตั้งสิ่งแวดล้อม ที่ประสบความสำเร็จ 1 ปี ต้องเสียเงินจำนวนนี้ต้องเดือดร้อนไม่ต่ำกว่า 3,000 ล้าน ตามแนันต่างด้าว 1 ปี ส่วนคนงานอื่นท\n",
      "achieved tok/s: 72.360953\n",
      "พันธมิตร หลายคนหลอกลวงห้คนไทยมาประชุมกันอีกครั้งหน่ง นายสนธิ ลิ้มทองกุล รองโษกพันธมิตร แลงงกรณีม็อบตัวแทนเครือข่ายปิรูป ไม่ประชาธิปัตย์ หรือเครือข่ายเตรียมเอกสารชี้ม็อบ 3 ข้อหาตามประมวลกหมายอาญา มาตรา 112 และประมวลกหมาย\n",
      "achieved tok/s: 72.857143\n",
      "กลุ่มนักกิจกรรมเพื่อประชาธิปไตย (คปร.) หรือกลุ่มนักกิจกรรมเพื่อสังคม กลุ่มแนวร่วมประชาธิปไตยต่อต้านเด็จการแห่งชาติ (นปช.) หรือกลุ่มชาติพันธุ์อื่น นองค์กรหรือองค์กรนักกิจกรรม องค์กรพันาเอกชน องค์กรภาคประชาสังคม สภาองค์กรชุมชน เอ็นจีโอด้านสิทธิมนุษย\n",
      "achieved tok/s: 72.054253\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    !./run out_thai/model.bin\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a047a7-3e2c-4a5c-9a11-d679b364895c",
   "metadata": {},
   "source": [
    "### Generate text using torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "17b01b3a-887f-4402-a559-b4671f590340",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "พันธมิตรฯ สั่งฟ้าดินแดง รับ 'นท.สุเทพ' ชี้ประเด็นสำคัญ แก้ปัญหา 'ชีวิตคนเสื้อแดง' ถูกจับ 'น้ำเสียพิษ' สั่งฟ้อง นายชัชวาลย์ สอดส่อง รองประธานคณะกรรมการประชาชนเพื่อแผ่นดินเลือกตั้ง (กปปส.) กล่าวถึงกรณีที่ นายสมเกียรติ พงษ์ไพบูลย์ รักษาการ ส.ส.นครรา\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "start = \"\"\n",
    "num_samples = 1 # number of samples to draw\n",
    "max_new_tokens = 256 # number of tokens generated in each sample\n",
    "temperature = 0.7 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 300\n",
    "\n",
    "t = Tokenizer()\n",
    "\n",
    "start_ids = t.encode(start, bos=True, eos=False)\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "# run generation\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(num_samples):\n",
    "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "            print(t.decode(y[0].tolist()))\n",
    "            print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb071fe3-ccf8-45bd-a57d-5c9a62f6a8b7",
   "metadata": {},
   "source": [
    "### Generate text in GGUF format using llama.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b945e3db-8d19-4da0-9555-ed84f9e42796",
   "metadata": {},
   "source": [
    "#### Convert from llama2c format to GGML format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "682f84fe-989a-4087-bae8-4e1eb3fc4b7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[malloc_weights:AK] Allocating [32000] x [288] = [9216000] float space for w->token_embedding_table\n",
      "[malloc_weights:AK] Allocating [6] x [288] = [1728] float space for w->rms_att_weight\n",
      "[malloc_weights:AK] Allocating [6] x [288] = [1728] float space for w->rms_ffn_weight\n",
      "[malloc_weights:AK] Allocating [6] x [288] x [288] = [497664] float space for w->wq\n",
      "[malloc_weights:AK] Allocating [6] x [288] x [288] = [497664] float space for w->wk\n",
      "[malloc_weights:AK] Allocating [6] x [288] x [288] = [497664] float space for w->wv\n",
      "[malloc_weights:AK] Allocating [6] x [288] x [288] = [497664] float space for w->wo\n",
      "[malloc_weights:AK] Allocating [6] x [768] x [288] = [1327104] float space for w->w1\n",
      "[malloc_weights:AK] Allocating [6] x [288] x [768] = [1327104] float space for w->w2\n",
      "[malloc_weights:AK] Allocating [6] x [768] x [288] = [1327104] float space for w->w3\n",
      "[malloc_weights:AK] Allocating [288] float space for w->rms_final_weight\n",
      "WARNING: Behavior may be unexpected when allocating 0 bytes for ggml_malloc!\n",
      "print_params: n_vocab: 32000\n",
      "print_params: n_ctx:   128\n",
      "print_params: n_embd:  288\n",
      "print_params: n_mult:  32\n",
      "print_params: n_head:  6\n",
      "print_params: n_ff:    768\n",
      "print_params: n_layer: 6\n",
      "print_params: n_rot:   48\n",
      "[init_model:GG] Allocating [288] x [32000] = [9216000] float space for model->tok_embeddings\n",
      "[init_model:GG] Allocating [288] float space for model->norm\n",
      "[init_model:GG] Allocating [288] x[32000] = [9216000] float space for model->output\n",
      "[init_model:GG] Allocating [288] x[288] = [82944] float space for layer.wq for [6] layers\n",
      "[init_model:GG] Allocating [288] x[288] = [82944] float space for layer.wk for [6] layers\n",
      "[init_model:GG] Allocating [288] x[288] = [82944] float space for layer.wv for [6] layers\n",
      "[init_model:GG] Allocating [288] x[288] = [82944] float space for layer.wo for [6] layers\n",
      "[init_model:GG] Allocating [288] float space for layer.ffn_norm for [6] layers\n",
      "[init_model:GG] Allocating [768] x[288] = [221184] float space for layer.w1 for [6] layers\n",
      "[init_model:GG] Allocating [288] x[768] = [221184] float space for layer.w2 for [6] layers\n",
      "[init_model:GG] Allocating [768] x[288] = [221184] float space for layer.w3 for [6] layers\n",
      "Saving llama.c model file out_thai/model.bin in ggml format at out_thai/llama2c-ggml.bin\n"
     ]
    }
   ],
   "source": [
    "!./convert-llama2c-to-ggml --copy-vocab-from-model ggml-vocab-llama.gguf --llama2c-model out_thai/model.bin --llama2c-output-model out_thai/llama2c-ggml.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a8779b1c-ec3a-4c4d-8149-e9643414824c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 2295 (87c91c07)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1709490150\n",
      "llama_model_loader: loaded meta data with 18 key-value pairs and 57 tensors from out_thai/llama2c-ggml.bin (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv   1:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv   2:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv   3:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv   4:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   5:                               general.name str              = llama\n",
      "llama_model_loader: - kv   6:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv   7:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv   8:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv   9:          tokenizer.ggml.seperator_token_id u32              = 4294967295\n",
      "llama_model_loader: - kv  10:            tokenizer.ggml.padding_token_id u32              = 4294967295\n",
      "llama_model_loader: - kv  11:                       llama.context_length u32              = 128\n",
      "llama_model_loader: - kv  12:                     llama.embedding_length u32              = 288\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 768\n",
      "llama_model_loader: - kv  14:                 llama.attention.head_count u32              = 6\n",
      "llama_model_loader: - kv  15:                          llama.block_count u32              = 6\n",
      "llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 48\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - type  f32:   57 tensors\n",
      "llm_load_vocab: bad special token: 'tokenizer.ggml.seperator_token_id' = 4294967295d, using default id -1\n",
      "llm_load_vocab: bad special token: 'tokenizer.ggml.padding_token_id' = 4294967295d, using default id -1\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 128\n",
      "llm_load_print_meta: n_embd           = 288\n",
      "llm_load_print_meta: n_head           = 6\n",
      "llm_load_print_meta: n_head_kv        = 6\n",
      "llm_load_print_meta: n_layer          = 6\n",
      "llm_load_print_meta: n_rot            = 48\n",
      "llm_load_print_meta: n_embd_head_k    = 48\n",
      "llm_load_print_meta: n_embd_head_v    = 48\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 288\n",
      "llm_load_print_meta: n_embd_v_gqa     = 288\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 768\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 128\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = all F32 (guessed)\n",
      "llm_load_print_meta: model params     = 24.41 M\n",
      "llm_load_print_meta: model size       = 93.11 MiB (32.00 BPW) \n",
      "llm_load_print_meta: general.name     = llama\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.02 MiB\n",
      "llm_load_tensors:        CPU buffer size =    93.11 MiB\n",
      "...........................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =     3.38 MiB\n",
      "llama_new_context_with_model: KV self size  =    3.38 MiB, K (f16):    1.69 MiB, V (f16):    1.69 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     2.57 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    63.06 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "main: warning: model was trained on only 128 context tokens (512 specified)\n",
      "\n",
      "system_info: n_threads = 8 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 256, n_keep = 1\n",
      "\n",
      "\n",
      " กำลังต่อต้านโรงเรียน ใต้ สำแดงราษ���ร ไม่อยากเป็นใคร สำหรับเด็กอาจมีแต่ความยุติธรรม ไม่สมควรใช้ “ผู้บริโภค” ที่ถูกปล้นชิง และจาบจ้วงส��าบันพระมหากษัตริย์ ด้วยเหตุนี้ กำลังอาจเป็นการกล่าวหา��นนามของคณะกรรมการ ความยุติธรรมและเสรีภาพ ซ��่งเป็นห\n",
      "llama_print_timings:        load time =      75.62 ms\n",
      "llama_print_timings:      sample time =      68.98 ms /   256 runs   (    0.27 ms per token,  3711.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    1498.93 ms /   256 runs   (    5.86 ms per token,   170.79 tokens per second)\n",
      "llama_print_timings:       total time =    1685.88 ms /   257 tokens\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main -m out_thai/llama2c-ggml.bin -n 256 -t 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76bb43c-02a0-4f9d-9947-bb1e89c087a0",
   "metadata": {},
   "source": [
    "#### Convert from torch checkpoint to GGUF format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81cd29f8-932a-4a23-a07c-1ae2e34e490f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model file out_thai/ckpt.pt\n",
      "params = Params(n_vocab=32000, n_embd=288, n_layer=6, n_ctx=4096, n_ff=768, n_head=2, n_head_kv=2, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('out_thai'))\n",
      "Found vocab files: {'tokenizer.model': PosixPath('tokenizer.model'), 'vocab.json': None, 'tokenizer.json': None}\n",
      "Loading vocab file 'tokenizer.model', type 'spm'\n",
      "Vocab info: <SentencePieceVocab with 32000 base tokens and 0 added tokens>\n",
      "Special vocab info: <SpecialVocab with 0 merges, special tokens unset, add special tokens unset>\n",
      "tok_embeddings.weight                            -> token_embd.weight                        | F32    | [32000, 288]\n",
      "layers.0.attention.wq.weight                     -> blk.0.attn_q.weight                      | F32    | [288, 288]\n",
      "layers.0.attention.wk.weight                     -> blk.0.attn_k.weight                      | F32    | [288, 288]\n",
      "layers.0.attention.wv.weight                     -> blk.0.attn_v.weight                      | F32    | [288, 288]\n",
      "layers.0.attention.wo.weight                     -> blk.0.attn_output.weight                 | F32    | [288, 288]\n",
      "layers.0.feed_forward.w1.weight                  -> blk.0.ffn_gate.weight                    | F32    | [768, 288]\n",
      "layers.0.feed_forward.w2.weight                  -> blk.0.ffn_down.weight                    | F32    | [288, 768]\n",
      "layers.0.feed_forward.w3.weight                  -> blk.0.ffn_up.weight                      | F32    | [768, 288]\n",
      "layers.0.attention_norm.weight                   -> blk.0.attn_norm.weight                   | F32    | [288]\n",
      "layers.0.ffn_norm.weight                         -> blk.0.ffn_norm.weight                    | F32    | [288]\n",
      "layers.1.attention.wq.weight                     -> blk.1.attn_q.weight                      | F32    | [288, 288]\n",
      "layers.1.attention.wk.weight                     -> blk.1.attn_k.weight                      | F32    | [288, 288]\n",
      "layers.1.attention.wv.weight                     -> blk.1.attn_v.weight                      | F32    | [288, 288]\n",
      "layers.1.attention.wo.weight                     -> blk.1.attn_output.weight                 | F32    | [288, 288]\n",
      "layers.1.feed_forward.w1.weight                  -> blk.1.ffn_gate.weight                    | F32    | [768, 288]\n",
      "layers.1.feed_forward.w2.weight                  -> blk.1.ffn_down.weight                    | F32    | [288, 768]\n",
      "layers.1.feed_forward.w3.weight                  -> blk.1.ffn_up.weight                      | F32    | [768, 288]\n",
      "layers.1.attention_norm.weight                   -> blk.1.attn_norm.weight                   | F32    | [288]\n",
      "layers.1.ffn_norm.weight                         -> blk.1.ffn_norm.weight                    | F32    | [288]\n",
      "layers.2.attention.wq.weight                     -> blk.2.attn_q.weight                      | F32    | [288, 288]\n",
      "layers.2.attention.wk.weight                     -> blk.2.attn_k.weight                      | F32    | [288, 288]\n",
      "layers.2.attention.wv.weight                     -> blk.2.attn_v.weight                      | F32    | [288, 288]\n",
      "layers.2.attention.wo.weight                     -> blk.2.attn_output.weight                 | F32    | [288, 288]\n",
      "layers.2.feed_forward.w1.weight                  -> blk.2.ffn_gate.weight                    | F32    | [768, 288]\n",
      "layers.2.feed_forward.w2.weight                  -> blk.2.ffn_down.weight                    | F32    | [288, 768]\n",
      "layers.2.feed_forward.w3.weight                  -> blk.2.ffn_up.weight                      | F32    | [768, 288]\n",
      "layers.2.attention_norm.weight                   -> blk.2.attn_norm.weight                   | F32    | [288]\n",
      "layers.2.ffn_norm.weight                         -> blk.2.ffn_norm.weight                    | F32    | [288]\n",
      "layers.3.attention.wq.weight                     -> blk.3.attn_q.weight                      | F32    | [288, 288]\n",
      "layers.3.attention.wk.weight                     -> blk.3.attn_k.weight                      | F32    | [288, 288]\n",
      "layers.3.attention.wv.weight                     -> blk.3.attn_v.weight                      | F32    | [288, 288]\n",
      "layers.3.attention.wo.weight                     -> blk.3.attn_output.weight                 | F32    | [288, 288]\n",
      "layers.3.feed_forward.w1.weight                  -> blk.3.ffn_gate.weight                    | F32    | [768, 288]\n",
      "layers.3.feed_forward.w2.weight                  -> blk.3.ffn_down.weight                    | F32    | [288, 768]\n",
      "layers.3.feed_forward.w3.weight                  -> blk.3.ffn_up.weight                      | F32    | [768, 288]\n",
      "layers.3.attention_norm.weight                   -> blk.3.attn_norm.weight                   | F32    | [288]\n",
      "layers.3.ffn_norm.weight                         -> blk.3.ffn_norm.weight                    | F32    | [288]\n",
      "layers.4.attention.wq.weight                     -> blk.4.attn_q.weight                      | F32    | [288, 288]\n",
      "layers.4.attention.wk.weight                     -> blk.4.attn_k.weight                      | F32    | [288, 288]\n",
      "layers.4.attention.wv.weight                     -> blk.4.attn_v.weight                      | F32    | [288, 288]\n",
      "layers.4.attention.wo.weight                     -> blk.4.attn_output.weight                 | F32    | [288, 288]\n",
      "layers.4.feed_forward.w1.weight                  -> blk.4.ffn_gate.weight                    | F32    | [768, 288]\n",
      "layers.4.feed_forward.w2.weight                  -> blk.4.ffn_down.weight                    | F32    | [288, 768]\n",
      "layers.4.feed_forward.w3.weight                  -> blk.4.ffn_up.weight                      | F32    | [768, 288]\n",
      "layers.4.attention_norm.weight                   -> blk.4.attn_norm.weight                   | F32    | [288]\n",
      "layers.4.ffn_norm.weight                         -> blk.4.ffn_norm.weight                    | F32    | [288]\n",
      "layers.5.attention.wq.weight                     -> blk.5.attn_q.weight                      | F32    | [288, 288]\n",
      "layers.5.attention.wk.weight                     -> blk.5.attn_k.weight                      | F32    | [288, 288]\n",
      "layers.5.attention.wv.weight                     -> blk.5.attn_v.weight                      | F32    | [288, 288]\n",
      "layers.5.attention.wo.weight                     -> blk.5.attn_output.weight                 | F32    | [288, 288]\n",
      "layers.5.feed_forward.w1.weight                  -> blk.5.ffn_gate.weight                    | F32    | [768, 288]\n",
      "layers.5.feed_forward.w2.weight                  -> blk.5.ffn_down.weight                    | F32    | [288, 768]\n",
      "layers.5.feed_forward.w3.weight                  -> blk.5.ffn_up.weight                      | F32    | [768, 288]\n",
      "layers.5.attention_norm.weight                   -> blk.5.attn_norm.weight                   | F32    | [288]\n",
      "layers.5.ffn_norm.weight                         -> blk.5.ffn_norm.weight                    | F32    | [288]\n",
      "norm.weight                                      -> output_norm.weight                       | F32    | [288]\n",
      "output.weight                                    -> output.weight                            | F32    | [32000, 288]\n",
      "Writing out_thai/ggml-model-f32.gguf, format 0\n",
      "Ignoring added_tokens.json since model matches vocab size without it.\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "[ 1/57] Writing tensor token_embd.weight                      | size  32000 x    288  | type F32  | T+   0\n",
      "[ 2/57] Writing tensor blk.0.attn_q.weight                    | size    288 x    288  | type F32  | T+   0\n",
      "[ 3/57] Writing tensor blk.0.attn_k.weight                    | size    288 x    288  | type F32  | T+   0\n",
      "[ 4/57] Writing tensor blk.0.attn_v.weight                    | size    288 x    288  | type F32  | T+   0\n",
      "[ 5/57] Writing tensor blk.0.attn_output.weight               | size    288 x    288  | type F32  | T+   0\n",
      "[ 6/57] Writing tensor blk.0.ffn_gate.weight                  | size    768 x    288  | type F32  | T+   0\n",
      "[ 7/57] Writing tensor blk.0.ffn_down.weight                  | size    288 x    768  | type F32  | T+   0\n",
      "[ 8/57] Writing tensor blk.0.ffn_up.weight                    | size    768 x    288  | type F32  | T+   0\n",
      "[ 9/57] Writing tensor blk.0.attn_norm.weight                 | size    288           | type F32  | T+   0\n",
      "[10/57] Writing tensor blk.0.ffn_norm.weight                  | size    288           | type F32  | T+   0\n",
      "[11/57] Writing tensor blk.1.attn_q.weight                    | size    288 x    288  | type F32  | T+   0\n",
      "[12/57] Writing tensor blk.1.attn_k.weight                    | size    288 x    288  | type F32  | T+   0\n",
      "[13/57] Writing tensor blk.1.attn_v.weight                    | size    288 x    288  | type F32  | T+   0\n",
      "[14/57] Writing tensor blk.1.attn_output.weight               | size    288 x    288  | type F32  | T+   0\n",
      "[15/57] Writing tensor blk.1.ffn_gate.weight                  | size    768 x    288  | type F32  | T+   0\n",
      "[16/57] Writing tensor blk.1.ffn_down.weight                  | size    288 x    768  | type F32  | T+   0\n",
      "[17/57] Writing tensor blk.1.ffn_up.weight                    | size    768 x    288  | type F32  | T+   0\n",
      "[18/57] Writing tensor blk.1.attn_norm.weight                 | size    288           | type F32  | T+   0\n",
      "[19/57] Writing tensor blk.1.ffn_norm.weight                  | size    288           | type F32  | T+   0\n",
      "[20/57] Writing tensor blk.2.attn_q.weight                    | size    288 x    288  | type F32  | T+   0\n",
      "[21/57] Writing tensor blk.2.attn_k.weight                    | size    288 x    288  | type F32  | T+   0\n",
      "[22/57] Writing tensor blk.2.attn_v.weight                    | size    288 x    288  | type F32  | T+   0\n",
      "[23/57] Writing tensor blk.2.attn_output.weight               | size    288 x    288  | type F32  | T+   0\n",
      "[24/57] Writing tensor blk.2.ffn_gate.weight                  | size    768 x    288  | type F32  | T+   0\n",
      "[25/57] Writing tensor blk.2.ffn_down.weight                  | size    288 x    768  | type F32  | T+   0\n",
      "[26/57] Writing tensor blk.2.ffn_up.weight                    | size    768 x    288  | type F32  | T+   0\n",
      "[27/57] Writing tensor blk.2.attn_norm.weight                 | size    288           | type F32  | T+   0\n",
      "[28/57] Writing tensor blk.2.ffn_norm.weight                  | size    288           | type F32  | T+   0\n",
      "[29/57] Writing tensor blk.3.attn_q.weight                    | size    288 x    288  | type F32  | T+   0\n",
      "[30/57] Writing tensor blk.3.attn_k.weight                    | size    288 x    288  | type F32  | T+   0\n",
      "[31/57] Writing tensor blk.3.attn_v.weight                    | size    288 x    288  | type F32  | T+   0\n",
      "[32/57] Writing tensor blk.3.attn_output.weight               | size    288 x    288  | type F32  | T+   0\n",
      "[33/57] Writing tensor blk.3.ffn_gate.weight                  | size    768 x    288  | type F32  | T+   0\n",
      "[34/57] Writing tensor blk.3.ffn_down.weight                  | size    288 x    768  | type F32  | T+   0\n",
      "[35/57] Writing tensor blk.3.ffn_up.weight                    | size    768 x    288  | type F32  | T+   0\n",
      "[36/57] Writing tensor blk.3.attn_norm.weight                 | size    288           | type F32  | T+   0\n",
      "[37/57] Writing tensor blk.3.ffn_norm.weight                  | size    288           | type F32  | T+   0\n",
      "[38/57] Writing tensor blk.4.attn_q.weight                    | size    288 x    288  | type F32  | T+   0\n",
      "[39/57] Writing tensor blk.4.attn_k.weight                    | size    288 x    288  | type F32  | T+   0\n",
      "[40/57] Writing tensor blk.4.attn_v.weight                    | size    288 x    288  | type F32  | T+   0\n",
      "[41/57] Writing tensor blk.4.attn_output.weight               | size    288 x    288  | type F32  | T+   0\n",
      "[42/57] Writing tensor blk.4.ffn_gate.weight                  | size    768 x    288  | type F32  | T+   0\n",
      "[43/57] Writing tensor blk.4.ffn_down.weight                  | size    288 x    768  | type F32  | T+   0\n",
      "[44/57] Writing tensor blk.4.ffn_up.weight                    | size    768 x    288  | type F32  | T+   0\n",
      "[45/57] Writing tensor blk.4.attn_norm.weight                 | size    288           | type F32  | T+   0\n",
      "[46/57] Writing tensor blk.4.ffn_norm.weight                  | size    288           | type F32  | T+   0\n",
      "[47/57] Writing tensor blk.5.attn_q.weight                    | size    288 x    288  | type F32  | T+   0\n",
      "[48/57] Writing tensor blk.5.attn_k.weight                    | size    288 x    288  | type F32  | T+   0\n",
      "[49/57] Writing tensor blk.5.attn_v.weight                    | size    288 x    288  | type F32  | T+   0\n",
      "[50/57] Writing tensor blk.5.attn_output.weight               | size    288 x    288  | type F32  | T+   0\n",
      "[51/57] Writing tensor blk.5.ffn_gate.weight                  | size    768 x    288  | type F32  | T+   0\n",
      "[52/57] Writing tensor blk.5.ffn_down.weight                  | size    288 x    768  | type F32  | T+   0\n",
      "[53/57] Writing tensor blk.5.ffn_up.weight                    | size    768 x    288  | type F32  | T+   0\n",
      "[54/57] Writing tensor blk.5.attn_norm.weight                 | size    288           | type F32  | T+   0\n",
      "[55/57] Writing tensor blk.5.ffn_norm.weight                  | size    288           | type F32  | T+   0\n",
      "[56/57] Writing tensor output_norm.weight                     | size    288           | type F32  | T+   0\n",
      "[57/57] Writing tensor output.weight                          | size  32000 x    288  | type F32  | T+   0\n",
      "Wrote out_thai/ggml-model-f32.gguf\n"
     ]
    }
   ],
   "source": [
    "!python convert.py out_thai/ --ctx 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "941cf935-f3d0-4c5b-8c7d-26c00fbb7af7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 2295 (87c91c07)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1709490174\n",
      "llama_model_loader: loaded meta data with 15 key-value pairs and 57 tensors from out_thai/ggml-model-f32.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 288\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 6\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 768\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 144\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 2\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 2\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 0\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - type  f32:   57 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 288\n",
      "llm_load_print_meta: n_head           = 2\n",
      "llm_load_print_meta: n_head_kv        = 2\n",
      "llm_load_print_meta: n_layer          = 6\n",
      "llm_load_print_meta: n_rot            = 144\n",
      "llm_load_print_meta: n_embd_head_k    = 144\n",
      "llm_load_print_meta: n_embd_head_v    = 144\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 288\n",
      "llm_load_print_meta: n_embd_v_gqa     = 288\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 768\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = all F32\n",
      "llm_load_print_meta: model params     = 24.41 M\n",
      "llm_load_print_meta: model size       = 93.11 MiB (32.00 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.02 MiB\n",
      "llm_load_tensors:        CPU buffer size =    93.11 MiB\n",
      "...........................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =     3.38 MiB\n",
      "llama_new_context_with_model: KV self size  =    3.38 MiB, K (f16):    1.69 MiB, V (f16):    1.69 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     2.57 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    63.06 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "\n",
      "system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.700\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 256, n_keep = 1\n",
      "\n",
      "\n",
      " กลุภาวิชัยธ์ (C) 2. หนประระเสงวดา จัยา 3) 1-1) 26(ผ้าและเขวันทีดัน้ำ) ครักรู) 25) 2. อ.ร. อ.พลสน้างจใหน.บรอำน.เมหนค.เทบ.16.ศิษผ่าวง 19.ส.เก.ธ.เลื่อ.ด. ชลีเย ��จัญ. 2 2 2.533) 24336. ผศ.สุพลวราคนัด.ทอ.2.3.เหญาญ. เก. 178889.1 2 โภา บัช. นาคมีะ. 2.14\n",
      "llama_print_timings:        load time =      12.29 ms\n",
      "llama_print_timings:      sample time =      73.69 ms /   256 runs   (    0.29 ms per token,  3474.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     800.91 ms /   256 runs   (    3.13 ms per token,   319.64 tokens per second)\n",
      "llama_print_timings:       total time =    1002.36 ms /   257 tokens\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main -m out_thai/ggml-model-f32.gguf -n 256 --temp 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f2d8bf-3460-4952-a9c6-ca0cccefd79c",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13714827-ab7d-4c85-8514-03cf5d806a53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
